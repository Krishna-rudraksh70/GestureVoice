# Gesture Voice: Text to Sign Language Gesture Converter App

### Overview
This app converts text input into sign language gestures, providing accessibility for communication in sign language across multiple languages. It uses advanced machine learning models and computer vision algorithms to generate accurate gestures corresponding to the input text.

### Features
1. Multi-Language Support: Converts text to sign language gestures in multiple languages.
2. Accurate Gesture Generation: Utilizes machine learning models for precise gesture creation.
3. User-Friendly Interface: Intuitive design for easy input and viewing of sign language gestures.
4. Accessibility: Enhances accessibility for users of sign language by facilitating communication through text.


# Sign Language Recognition with LSTM Model
Welcome to the Sign Language Recognition project using Long Short-Term Memory (LSTM) models. This repository contains all the necessary files and code to perform sign language recognition using a deep learning approach.

## Repository Overview
This repository is structured as follows:

- lstm/: This folder contains all LSTM-related code and files.
- Jupyter notebooks for training and running the model.

## Prerequisites

Ensure you have the following libraries installed before running the notebooks:

- OpenCV (cv2)
- MediaPipe
- OS
- Time
- Matplotlib
- Pandas
- NumPy

You can install these libraries using pip:

bash
pip install opencv-python mediapipe matplotlib pandas numpy


## Setup Instructions

1. *Clone the Repository*: Clone the repository to your local machine using the following command:

    bash
    git clone https://github.com/Krishna-rudraksh70/GestureVoice.git
    

2. *Navigate to the LSTM Folder*: Change your directory to the lstm folder:

    bash
    cd GestureVoice/lstm
    

3. *Run the Jupyter Notebook*: Open the Jupyter notebook in the lstm folder and execute the code cells one by one. This will guide you through the process of loading the data, training the model, and running the predictions.

    jupyter notebook
    

    - Navigate to the notebook file in the Jupyter interface and open it.
    - Run each code cell sequentially. The notebook will prompt you for any necessary permissions, such as camera access.

## Important Notes

- *Camera Access*: The notebook will request camera access to capture gesture inputs. Ensure your camera is functional and permissions are granted.
- *Sequential Execution*: It is crucial to run the cells in the order they appear in the notebook to avoid any errors.

## Usage

Once the setup is complete and the notebook is running, you can use the system to recognize sign language gestures through your webcam. The trained LSTM model will process the video frames and provide real-time gesture recognition.

## Contributions

Contributions to this project are welcome. Feel free to fork the repository, make improvements, and submit pull requests.

[Krishna](https://github.com/Krishna-rudraksh70)
[Navya](https://github.com/Navya-Malik)

## License

This project is licensed under the MIT License. See the [LICENSE](../LICENSE) file for more details.

## Acknowledgements

This project utilizes several open-source libraries, and we acknowledge their contributions.

For any questions or issues, please open an issue on the repository or contact the maintainer.

Happy coding!
